# Kaggle-Reading-Group

### Universal Sentence Encoder (Part 2) | Kaggle

Join Kaggle Data Scientist Rachael as she reads through paper "Universal Sentence Encoder" by Cer et al. (Unpublished.) 

Link to paper: https://arxiv.org/pdf/1803.11175.pdf


### Generating Long Sequences with Sparse Transformers (Part 3)| Kaggle

Join Kaggle Data Scientist Rachael as she reads through an NLP paper! Today's paper is "Generating Long Sequences with Sparse Transformers" (Child et al, unpublished). 

You can find a copy here: https://arxiv.org/pdf/1904.10509.pdf


### Multi-Task DNNs for Natural Language Understanding (Part 3) | Kaggle

Join Kaggle Data Scientist Rachael as she reads through an NLP paper! Today's paper is "Multi-Task Deep Neural Networks for Natural Language Understanding " (Liu et al, unpublished). 

You can find a copy here: https://arxiv.org/pdf/1901.11504.pdf


### Dissecting contextual word embeddings (Part 4) | Kaggle

Join Kaggle Data Scientist Rachael as she reads through an NLP paper! Today's paper is "Dissecting contextual word embeddings: Architecture and representation" (Peters et al, 2018). 

You can find a copy here: https://aclweb.org/anthology/D18-1179

### Probing the Need for Visual Context in Multimodal Machine Translation| Kaggle

Join us for a special Kaggle Days edition of the Kaggle reading group! We'll be reading the recently-annouced best short paper from NAACL 2019; "Probing the Need for Visual Context in Multimodal Machine Translation". 

You can find a copy here: https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf

### Language Models are Unsupervised Multitask Learners (GPT-2, Part 3) | Kaggle

Join Kaggle Data Scientist Rachael as she reads through an NLP paper! Today's paper is "Language Models are Unsupervised Multitask Learners" (Radford et al, unpublished). 

You can find a copy here: https://d4mucfpksywv.cloudfront.net/b... 


### Bidirectional Encoder Representations from Transformers (aka BERT) (Part 4)

Join Kaggle Data Scientist Rachael as she reads through an NLP paper! Today's paper is "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding" (Devlin et al, unpublished). 

You can find a copy here: https://arxiv.org/pdf/1810.04805.pdf


