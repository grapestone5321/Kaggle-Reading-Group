# Kaggle-Reading-Group

## Rachael | Kaggle

Rachael was leaving Kaggle for new opportunities in 2020

### 1. Learning from Dialogue after Deployment | Kaggle

Streamed live on Oct 31, 2019

The paper is "Learning from Dialogue after Deployment: Feed Yourself, Chatbot!" by Hancock et al, 2019. (Published at ACL 2019 in Venice.)

Link to paper 1.: https://www.aclweb.org/anthology/P19-1358.pdf



### 2. EfficientNet (Part 2) | Kaggle

EfficientNet (Tan & Le, 2019) was published at ICML 2019. The paper proposes a new family of models that are both smaller and faster to train than traditional convolutional neural networks.

Link to paper 2.: http://proceedings.mlr.press/v97/tan19a/tan19a.pdf

### 3. EfficientNet (Part 1) | Kaggle


### 4. Deep Learning for Symbolic Mathematics (Part 2) | Kaggle

The paper is "Deep Learning for Symbolic Mathematics", (anonymous, submitted to ICLR 2020).

Link to the paper 3.: https://openreview.net/pdf?id=S1eZYeHFDS

### 5. Deep Learning for Symbolic Mathematics (Part 1) | Kaggle



### 6. Weight Agnostic Neural Networks (Part 2) | Kaggle

The paper is "Weight Agnostic Neural Networks" by Gaier & Ha from NeurIPS 2019.

Link to paper 4.: https://arxiv.org/pdf/1906.04358.pdf

### 7. Weight Agnostic Neural Networks (Part 1) | Kaggle


### 8. Probing Neural Network Comprehension of Natural Language Arguments (Part 2) | Kaggle 

BERT (which we read the paper for earlier) has had really impressive success on a number of NLP tasks... but how well is it really capturing the structures of natural language? 

We're starting off on  "Probing Neural Network Comprehension of Natural Language Arguments" (Niven & Kao, 2019). 
 
Link to the paper 5.: https://www.aclweb.org/anthology/P19-1459.pdf
 
### 9. Probing Neural Network Comprehension of Natural Language Arguments (Part 1) | Kaggle



### 10. An Open Source AutoML Benchmark | Kaggle

This week we're starting a new paper: An Open Source AutoML Benchmark by Gijsbers et al from the 2019 ICML Workshop on Automated Machine Learning.

Link to the paper 6.: https://www.automl.org/wp-content/uploads/2019/06/automlws2019_Paper45.pdf


### 11. XLNet (Part 4) | Kaggle

This week we're starting a new paper in the Kaggle reading group: XLNet: Generalized Autoregressive Pretraining for Language Understanding (Yang et al, unpublished). 

You can read the paper7.: https://arxiv.org/pdf/1906.08237.pdf

### 12. XLNet (Part 3) | Kaggle

### 13. XLNet (Part 2) | Kaggle

### 14. XLNet (Part 1) | Kaggle






### 15. Universal Sentence Encoder (Part 2) | Kaggle

Join Kaggle Data Scientist Rachael as she reads through paper "Universal Sentence Encoder" by Cer et al. (Unpublished.) 

Link to paper 8.: https://arxiv.org/pdf/1803.11175.pdf


### 16. Universal Sentence Encoder (Part 1) | Kaggle



### 17. Generating Long Sequences with Sparse Transformers (Part 3)| Kaggle

Join Kaggle Data Scientist Rachael as she reads through an NLP paper! Today's paper is "Generating Long Sequences with Sparse Transformers" (Child et al, unpublished). 

You can find a copy here 9.: https://arxiv.org/pdf/1904.10509.pdf


### 18. Generating Long Sequences with Sparse Transformers (Part 2)| Kaggle

### 19. Generating Long Sequences with Sparse Transformers (Part 1)| Kaggle




### 20. Multi-Task DNNs for Natural Language Understanding (Part 3) | Kaggle

Join Kaggle Data Scientist Rachael as she reads through an NLP paper! Today's paper is "Multi-Task Deep Neural Networks for Natural Language Understanding " (Liu et al, unpublished). 

You can find a copy here 10.: https://arxiv.org/pdf/1901.11504.pdf

### 21. Multi-Task DNNs for Natural Language Understanding (Part 2) | Kaggle

### 22. Multi-Task DNNs for Natural Language Understanding (Part 1) | Kaggle


### 23. Dissecting contextual word embeddings (Part 4) | Kaggle

Join Kaggle Data Scientist Rachael as she reads through an NLP paper! Today's paper is "Dissecting contextual word embeddings: Architecture and representation" (Peters et al, 2018). 

You can find a copy here 11.: https://www.aclweb.org/anthology/D18-1179.pdf

### 24. Dissecting contextual word embeddings (Part 3) | Kaggle

### 25. Dissecting contextual word embeddings (Part 2) | Kaggle

### 26. Dissecting contextual word embeddings (Part 1) | Kaggle



### 27. Probing the Need for Visual Context in Multimodal Machine Translation| Kaggle

Join us for a special Kaggle Days edition of the Kaggle reading group! We'll be reading the recently-annouced best short paper from NAACL 2019; "Probing the Need for Visual Context in Multimodal Machine Translation". 

You can find a copy here 12.: https://arxiv.org/pdf/1903.08678.pdf




### 28. Language Models are Unsupervised Multitask Learners (GPT-2, Part 3) | Kaggle

Join Kaggle Data Scientist Rachael as she reads through an NLP paper! Today's paper is "Language Models are Unsupervised Multitask Learners" (Radford et al, unpublished). 


You can find a copy here 13.: https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf

### 29. Language Models are Unsupervised Multitask Learners (GPT-2, Part 2) | Kaggle

### 30. Language Models are Unsupervised Multitask Learners (GPT-2, Part 1) | Kaggle

### 31. Bidirectional Encoder Representations from Transformers (aka BERT) (Part 4) | Kaggle

Join Kaggle Data Scientist Rachael as she reads through an NLP paper! Today's paper is "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding" (Devlin et al, unpublished). 

You can find a copy here 14.: https://arxiv.org/pdf/1810.04805.pdf


### 32. Bidirectional Encoder Representations from Transformers (aka BERT) (Part 3) | Kaggle

### 33. Bidirectional Encoder Representations from Transformers (aka BERT) (Part 2) | Kaggle

### 34. Bidirectional Encoder Representations from Transformers (aka BERT) (Part 1) | Kaggle





### 35. Attention is all You Need (Pt. 3) | Kaggle

Join Kaggle Data Scientist Rachael as she reads through an NLP paper! Today's paper is "Attention is All You Need" (Vaswani et al 2017). 

You can find a copy here 15.: https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf


### 36. Attention is all You Need (Pt. 2) | Kaggle


### 37. Attention is all You Need (Pt. 1) | Kaggle


### 38. Neural Networks and Neural Language Models (Part 3: Embeddings) | Kaggle

The paper is the chapter "Neural Networks and Neural Language Models" from "Speech and Language Processing" by Daniel Jurafsky & James H. Martin. This chapter is new to the currently-in-progress edition of the book, so even if you've read it previously, this should be new. 

You can find a link to the paper here 16.: https://web.stanford.edu/~jurafsky/slp3/7.pdf


### 39. Neural Networks and Neural Language Models (Part 2: Training) | Kaggle

### 40. Neural Networks and Neural Language Models (Part 1) | Kaggle


### 41. Understanding Social Connections from Unstructured Text | Kaggle

Today's paper is "Know Who Your Friends Are: Understanding Social Connections from Unstructured Text", by Deleris et al. 

Link to paper 17.: https://www.aclweb.org/anthology/N18-5016.pdf

### 42. Learning from Dialogue after Deployment (Part 2) | Kaggle

This week we continue with the paper "Learning from Dialogue after Deployment:
Feed Yourself, Chatbot!" by Hancock et al, 2019. (Published at ACL 2019 in Venice.)

Link to paper 18.: https://www.aclweb.org/anthology/P19-1358.pdf


### 43. On NMT Search Errors and Model Errors: Cat Got Your Tongue? (Part 2) | Kaggle

This week we'll be starting a new paper: "On NMT Search Errors and Model Errors: Cat Got Your Tongue?" by Felix Stahlber and Bill Byrne, published at EMNLP 2019. 

You can follow along with the paper here 19.: https://www.aclweb.org/anthology/D19-1331.pdf

### 44. On NMT Search Errors and Model Errors: Cat Got Your Tongue? (Part 1) | Kaggle


### 45. Blank 



